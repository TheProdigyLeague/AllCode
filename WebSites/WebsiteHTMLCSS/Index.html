<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name ="description" content="This is Gabe's website">
		<title>Gabriel's Website</title>
	</head>
	<body style="background-color: tan;">
		<header>
		</header>
		<main>
			<h1>Gabriel's Website</h1>
			<hr/>  <!-- this is a horazontal rule. Makes a line on screen -->
			<br/>
			<p><b>Welcome</b> to the <i><u>website!</u></i></p>
			<p><big>We</big> have <small>so</small> much in store (:</p>
			<!-- <br/> -->
			<p style="background-color: green; color: tan;"> H<sub>2</sub>O   or even 69<sup>2</sup> </p> 
			<article>
				<section>
					<h3 style="color: green;">From machine learning website</h3>
					<p style="color: green; background-color: tan;">
						5. What’s your learning rate?
						In practice, you should either keep the pre-trained parameters fixed (ie. use the pre-trained models as feature extractors) as or tune them with a fairly small learning in order to not unlearn everything in the original model.
						6. Is there a difference in how you use optimizations like batch normalization or dropout, especially between training mode and inference mode?
						As Curtis’ post claims:
						Keras models using batch normalization can be unreliable. For some models, forward-pass evaluations (with gradients supposedly off) still result in weights changing at inference time. (See 5)
						But why is this the case?
						According to Vasilis Vryniotis, Principal Data Scientist at Expedia, who first identified the issue with the frozen batch normalization layer in Keras (see Vasilis’ PR here and detailed blog post here): 
					</p>
					<aside style="color: blue;">
						Maybe I would put an advertisement here?
						<a href="https://www.google.com" target="_blank">
							<h4>Google's Homepage</h4>
						</a>
						<ul>
							<li><a href="Page2.html">Link to Media</a></li>
							<li><a href="Page3.html">Text Input & Tables</a></li>
						</ul>
					</aside>
				</section>
			</article>
		</main>
		<footer>
		</footer>
	</body>
</html>